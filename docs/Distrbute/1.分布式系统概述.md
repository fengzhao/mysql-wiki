
# 前言

MIT 6.824 分布式系统，MIT的分布式系统课程就是由不同论文组成
https://github.com/chaozh/MIT-6.824
https://mit-public-courses-cn-translatio.gitbook.io/mit6-824

6.5840（在 2023 年之前被称为 6.824）是 MIT 的一门研究生核心课程。每节课会精读一篇分布式系统领域的经典论文，以此传授
分布式系统设计与实现的重要原则和关键技术。课程包含 4 个编程作业，循序渐进地实现一个基于 Raft 共识算法的 KV-store 框架。

为什么要分布式

当数据量和计算量非常大时，需要将计算机规模化（Scale）。规模化有两个方向：垂直（scale up）和水平（scale out）。

- 垂直扩展指提升单个计算机的性能（比如说现在动辄几T内存用来跑超大的数据库系统）；

- 水平扩展指将计算量分摊到很多的计算机上，通过增加计算机的数量来暴力破解，又称为分布式系统。

在计算机发明的初期主要依靠竖直提升，而从上世纪进入互联网爆发期之后，由于数据量猛增，单机的性能跟不上数据量的增长，所以互联网公司开始探索另一条路：水平规模化，而其中尤以谷歌出名。

比如谷歌创业初期购置了大量便宜的x86服务器来搭建分布式系统撑起谷歌的全球流量。

垂直化的例子包括计算机 CPU 缓存、指令预测、提升芯片执行指令的速度、提升 RAM 容量、提升硬盘读写速度等等，既有软件层面又有硬件层面，CMU 教材 CSAPP 里面提供了大量的例子，感兴趣的读者可以深入阅读此书；

水平化的例子简单的比如使用 NGINX 进行负载均衡，复杂的有基于 Map Reduce 进行分布式计算、Redis 集群模式、ZooKeeper 或者 etcd 这样的分布式 KV 存储等等，更多是在软件层面做工作，熟悉 web 服务器编程的读者一定不会陌生。


分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像是单个相关系统。
首先，分布式系统相对来说比较强大，至少由数台计算机组成。以阿里云、腾讯云、华为云等服务商为例，他们的数据中心计算机规模都在万台以上；
其次，虽然分布式系统很强大，但是“深藏不露”，对用户来说，根本感觉不到计算机集群的存在，与单机无异。

从进程角度看，两个程序分别运行在两台计算机上，它们相互协作完成同一个服务（或者功能），从理论上讲，这两个程序所组成的系统，就可以称作是“分布式系统”。
当然，这个两个程序可以是不同的程序，也可以是相同的程序。如果是相同的程序，我们又可以称之为“集群”。

对于数据存储领域，当数据量或者请求流量大到一定程度后，就必然会引入分布式。比如 Redis，虽然其单机性能十分优秀，但是因为下列原因时，也不得不引入集群。

- 单机无法保证高可用，需要引入多实例来提供高可用性
- 单机能够提供高达 8W 左右的QPS，再高的QPS则需要引入多实例
- 单机能够支持的数据量有限，处理更多的数据需要引入多实例；
- 单机所处理的网络流量已经超过服务器的网卡的上限值，需要引入多实例来分流。

有集群，集群往往需要维护一定的元数据，比如实例的ip地址，缓存分片的 slots 信息等，所以需要一套分布式机制来维护元数据的一致性。这类机制一般有两个模式：分散式和集中式


### 分散式
分散式机制将元数据存储在部分或者所有节点上，不同节点之间进行不断的通信来维护元数据的变更和一致性。Redis Cluster，Consul 等都是该模式。

redis集群中每台机器上都保存了一份元数据，如果有节点出现了元数据变更会不断的新的元数据发给其他节点，让其他节点也进行元数据变更。

分散式（Decentralized）元数据管理机制的主要特点之一就是集群中的每个节点通常都能够对外提供服务，特别是关于元数据的读操作。

**读操作**

- 所有节点都可以接受客户端的读请求并直接响应。 这极大地提高了系统的读扩展性（Read Scalability），因为客户端可以将请求分散到任意节点，避免了单一瓶颈。

- 高可用性： 只要有任何一个节点存活，就能提供元数据查询服务。


**写操作**

写操作（元数据的变更）则稍微复杂一些，通常有两种处理方式：

- 纯分散式: 任何节点都可以发起写操作。该节点接收请求后，会将更新通过 Gossip 协议 或其他同步机制传播给集群中的其他所有节点。

- 代理转发：节点接收到写请求后，如果它不是该元数据的主节点（Master/Leader），它会将请求转发给负责该数据的节点，由主节点执行更新并负责同步。比如redis的mv机制


### 集中式

集群中设置少数几个节点作为专门的协调者（Coordinator）来存储和管理所有元数据。所有数据变更请求都必须通过这个协调者小组来完成。典型代表技术有 ETCD 、ZooKeeper 等。




可扩展性Scalability

两台计算机构成的系统如果有两倍性能或者吞吐，就是所说的可扩展性。

可用性Availability

一台计算机正常工作很长时间并不少见。然而如果你通过数千台计算机构建你的系统，那么即使每台计算机可以稳定运行一年，对于1000台计算机也意味着平均每天会有3台计算机故障。
大型分布式系统中有一个大问题，那就是一些很罕见的问题会被放大。例如在我们的1000台计算机的集群中，总是有故障，要么是机器故障，要么是运行出错，要么是运行缓慢，要么是执行错误的任务。
一个更常见的问题是网络，在一个有1000台计算机的网络中，会有大量的网络电缆和网络交换机，所以总是会有人踩着网线导致网线从接口掉出，或者交换机风扇故障导致交换机过热而不工作。
在一个大规模分布式系统中，各个地方总是有一些小问题出现。所以大规模系统会将一些几乎不可能并且你不需要考虑的问题，变成一个持续不断的问题。
某些系统经过精心的设计，这样在特定的错误类型下，系统仍然能够正常运行，仍然可以像没有出现错误一样，为你提供完整的服务。

比如，你构建了一个有两个拷贝的多副本系统，其中一个故障了，另一个还能运行。当然如果两个副本都故障了，你的系统就不再有可用性。
所以，可用系统通常是指，在特定的故障范围内，系统仍然能够提供服务，系统仍然是可用的。如果出现了更多的故障，系统将不再可用。

另一种容错特性是自我可恢复性（recoverability）。这里的意思是，如果出现了问题，服务会停止工作，不再响应请求，之后有人来修复，并且在修复之后系统仍然可以正常运行，就像没有出现过问题一样。
这是一个比可用性更弱的需求，因为在出现故障到故障组件被修复期间，系统将会完全停止工作。但是修复之后，系统又可以完全正确的重新运行，所以可恢复性是一个重要的需求。


**可靠性（Reliability）**

人们对于一个东西是否可靠，都有一个直观的想法，人们对可靠软件的典型期望包括：

- 应用程序表现出用户所期望的功能
- 允许用户犯错，允许用户以出乎意料的方式使用软件
- 在预期的负载和数据量下，性能满足要求
- 系统能防止未经授权的访问和滥用


如果这些叠在一起就意味着「正确工作」，那么可以把可靠性粗略理解为「即使出现问题，也能继续正确工作」。
造成错误的原因叫做「故障（fault）」，能预料并应对故障的系统特性可称为「容错（fault-tolerant）」或「韧性（resilient）」。
这里的容错一词可能会产生误导，因为它暗示着系统可以容忍所有可能的错误，但在实际中这是不可能的，因为任何性质都是有极限的。举个栗子，如果整个地球都毁灭了，我们的系统还能保证高可靠吗？
如果想保证的话，那么只能将服务器部署在其它星球上了。所以在讨论容错时，只有谈论特定类型的错误才有意义。

然后还要注意的是「故障」，我们常常会说系统故障了、程序出故障了之类的，它是造成错误的原因，但是「故障（fault）」不等同于「失效（failure）」
故障通常定义为系统的一部分状态偏离其标准，而失效则是系统作为一个整体停止向用户提供服务，所以当系统出现故障时，如果系统不具备容错或者出故障时不进行修复，那么就可能造成系统失效。
所以一个好的分布式系统需要设计出优秀的容错机制来防止系统因「故障」而「失效」，可能有人觉得如果能确保系统不出故障的话，是不是就不容设计容错机制了？
理论上是这样的，但现实是不允许的，一个分布式系统出故障的概率不可能降低到零。
因为在生产环境中各种情况都有可能出现，比如某个节点的磁盘坏掉了，网络连接出问题了，安装新服务器的时候哪个小伙伴不小心把网线踢掉了等等。
这些都属于不在控制范围之内的故障，尽管发生的可能性很小，但确实有可能会发生。
而一旦发生，就意味着整个分布式系统出故障了，但是出现故障并不代表这个分布式系统设计的不好，好的分布式系统指的是在出现故障（预期之内）时能够很好地容错，并仍然能够正常工作。

不过反直觉的是，在这类容错系统中，通过故意触发来提高故障率也是有意义的，比如在没有告警的情况下随机地杀死进程。
因为许多高危漏洞实际上是由糟糕的错误处理导致的，因为我们可以通过故意引发故障来确保容错机制不断运行并接收考验，从而提高故障自然发生时系统能正确处理的信心。
虽然故障无法百分百避免，或者说系统不可能百分百不出错，因此相比「阻止错误（prevent error）」，我们更倾向于「容忍错误」。
不过也有「预防胜于治疗」的情况，如果一个错误我们百分百无法容忍的话，那么只能事先尽最大努力确保它不发生，比如安全问题。如果某个攻击者破坏了系统，并获取了敏感数据，这种事情显然是撤销不了的，无法做容错。



硬件故障
当想到系统失效的原因时，硬件故障（hardware faults）总会第一个进入脑海，例如硬盘崩溃、内存出错、机房断电、有人拔错网线等等。任何与大型数据中心打过交道的人都会告诉你：一旦你拥有很多机 器，这些事情总会发生。
据报道称，硬盘的平均无故障时间（MTTF，mean time to failure）约为 10 到 50 年，因此从数学期望上讲，在拥有 10000 个磁盘的存储集群上，平均每天会有 1 个磁盘出故障。

为了减少系统的故障率，第一反应通常都是增加单个硬件的冗余度，例如磁盘可以组建 RAID，服务器可能有双路电源和热插拔 CPU，数据中心可能有电池和柴油发电机作为后备电源，某个组件挂掉时冗余组件可以立刻接管。
这种方法虽然不能完全防止由硬件问题导致的系统失效，但它简单易懂，通常也足以让机器不间断运行很多年。

直到最近，硬件冗余对于大多数应用来说已经足够了，它使单台机器完全失效变得相当罕见。
只要你能快速地把备份恢复到新机器上，故障停机时间对大多数应用而言都算不上灾难性的，但实际上只有少量高可用性至关重要的应用才会要求有多套硬件冗余。

不过随着数据量和应用计算需求的增加，越来越多的应用开始大量使用机器，这会相应地增加硬件故障率。
此外在一些云平台，如亚马逊AWS中，虚拟机实例不可用却没有任何警告也是很常见的，因为云平台的设计就是优先考虑灵活性（flexibility）和弹性（elasticity），而不是单机可靠性。

如果在硬件冗余的基础上进一步引入软件容错机制，那么系统在容忍整个（单台）机器故障的道路上就更进一步了。
这样的系统也有运维上的便利，例如：如果需要重启机器（例如应用操作系统安全补丁），单服务器系统就需要计划停机，而允许机器失效的系统则可以一次修复一个节点，无需整个系统停机。

软件错误
我们通常认为硬件故障是随机的、相互独立的：一台机器的磁盘失效并不意味着另一台机器的磁盘也会失效。
大量硬件组件不可能同时发生故障，除非它们存在比较弱的相关性（同样的原因导致关联性错误，例如服务器机架的温度，机架温度过高可能导致一批节点不可用）。
另一类错误是内部的系统性错误（systematic error），这类错误难以预料，而且因为是跨节点相关的，所以比起不相关的硬件故障往往可能造成更多的系统失效，举个栗子：
接受特定的错误输入，便导致所有应用服务器实例崩溃的 BUG，例如 2012 年 6 月 30 日的闰秒，由于 Linux 内核中的一个错误，许多应用同时挂掉了
失控进程会占用一些共享资源，包括 CPU 时间片、内存、磁盘空间或网络带宽
系统依赖的服务变慢，没有响应，或者开始返回错误的响应
级联故障，一个组件中的小故障触发另一个组件中的故障，进而触发更多的故障
导致这类软件故障的 BUG 通常会潜伏很长时间，直到被异常情况触发为止。这种情况意味着软件对其环境做出了某种假设：虽然这种假设通常来说是正确的，但由于某种原因最后不再成立了。

虽然软件中的系统性故障没有速效药，但我们还是有很多小办法，例如：仔细考虑系统中的假设和交互；彻底的测试；进程隔离；允许进程崩溃并重启；测量、监控并分析生产环境中的系统行为。
如果系统能够提供一些保证（例如在一个消息队列中，进入与发出的消息数量相等），那么系统就可以在运行时不断自检，并在出现差异（discrepancy）时报警。







